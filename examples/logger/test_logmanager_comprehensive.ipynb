{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ae2614",
   "metadata": {},
   "source": [
    "# LogManager Comprehensive Testing Notebook\n",
    "\n",
    "This notebook provides comprehensive testing for the LogManager class, covering:\n",
    "- Basic logging features and CRUD operations for loggers and handlers\n",
    "- Testing duplicate file warnings\n",
    "- HDFS copy functionality\n",
    "- Distributed coordination features\n",
    "\n",
    "## Prerequisites\n",
    "- LogManager class properly installed\n",
    "- HDFS access (or local file system for testing)\n",
    "- Required dependencies installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f506ad3",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06217fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n",
      "Python version: 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\n",
      "Working directory: c:\\Users\\Lisa Tan\\Desktop\\Projects\\utilities\\examples\\logger\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Import LogManager\n",
    "from utilities import LogManager\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ff9bc",
   "metadata": {},
   "source": [
    "## 2. Basic Logging Configuration and CRUD Operations\n",
    "\n",
    "Test basic LogManager functionality including creating, reading, updating, and deleting loggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f88de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file not provided, initializing logger with class default config.\n",
      "HDFS copy enabled (default behavior)\n",
      "Signal handlers registered\n",
      "✅ LogManager created successfully\n",
      "Config path: C:\\Users\\Lisa Tan\\Desktop\\Projects\\utilities\\src\\main\\logger\\_default_logger_config.yaml\n",
      "Current handlers: ['handler_file', 'handler_console']\n",
      "Current loggers: ['default_task']\n"
     ]
    }
   ],
   "source": [
    "# Create a LogManager instance\n",
    "log_manager = LogManager()\n",
    "\n",
    "print(\"✅ LogManager created successfully\")\n",
    "print(f\"Config path: {log_manager._config_path}\")\n",
    "print(f\"Current handlers: {list(log_manager._handlers_map.keys())}\")\n",
    "print(f\"Current loggers: {list(log_manager._loggers_map.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0842f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING LOGGER CRUD OPERATIONS ===\n",
      "✅ Created logger 'test_app'\n",
      "✅ Created logger 'test_database'\n",
      " \u001b[32m2025-08-22 22:03:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[31mtest_app\u001b[0m | \u001b[36m2872554959.py    | <module>\u001b[0m : 26 - \u001b[37mThis is a test message from app logger\u001b[0m\n",
      " \u001b[32m2025-08-22 22:03:50\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[31mtest_app\u001b[0m | \u001b[36m2872554959.py    | <module>\u001b[0m : 27 - \u001b[37mDebug message from app logger\u001b[0m\n",
      "✅ Successfully retrieved and used loggers\n",
      "Current loggers: ['default_task', 'test_app', 'test_database']\n"
     ]
    }
   ],
   "source": [
    "# Test Logger CRUD Operations\n",
    "print(\"=== TESTING LOGGER CRUD OPERATIONS ===\")\n",
    "\n",
    "# CREATE: Add multiple loggers\n",
    "try:\n",
    "    # Add test loggers with different configurations\n",
    "    log_manager.add_logger(\"test_app\", [\n",
    "        {\"handler\": \"handler_console\", \"level\": \"DEBUG\"},\n",
    "        {\"handler\": \"handler_file\", \"level\": \"INFO\"}\n",
    "    ])\n",
    "    print(\"✅ Created logger 'test_app'\")\n",
    "    \n",
    "    log_manager.add_logger(\"test_database\", [\n",
    "        {\"handler\": \"handler_file\", \"level\": \"WARNING\"}\n",
    "    ])\n",
    "    print(\"✅ Created logger 'test_database'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating loggers: {e}\")\n",
    "\n",
    "# READ: Get and use loggers\n",
    "try:\n",
    "    app_logger = log_manager.get_logger(\"test_app\")\n",
    "    db_logger = log_manager.get_logger(\"test_database\")\n",
    "    \n",
    "    app_logger.info(\"This is a test message from app logger\")\n",
    "    app_logger.debug(\"Debug message from app logger\")\n",
    "    db_logger.warning(\"Warning message from database logger\")\n",
    "    \n",
    "    print(\"✅ Successfully retrieved and used loggers\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error using loggers: {e}\")\n",
    "\n",
    "print(f\"Current loggers: {list(log_manager._loggers_map.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72191e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UPDATING LOGGER CONFIGURATION ===\n",
      " \u001b[32m2025-08-22 22:03:50\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31mtest_app\u001b[0m | \u001b[36m1768015933.py    | <module>\u001b[0m : 13 - \u001b[37mThis ERROR should appear\u001b[0m\n",
      "✅ Successfully updated logger configuration\n",
      "\n",
      "=== DELETING LOGGER ===\n",
      "✅ Successfully removed 'test_database' logger\n",
      "Remaining loggers: ['default_task', 'test_app']\n"
     ]
    }
   ],
   "source": [
    "# UPDATE: Modify existing logger configuration\n",
    "print(\"\\n=== UPDATING LOGGER CONFIGURATION ===\")\n",
    "\n",
    "try:\n",
    "    # Update the test_app logger to only use console handler\n",
    "    log_manager.update_logger(\"test_app\", [\n",
    "        {\"handler\": \"handler_console\", \"level\": \"ERROR\"}\n",
    "    ])\n",
    "    \n",
    "    # Test the updated logger\n",
    "    updated_logger = log_manager.get_logger(\"test_app\")\n",
    "    updated_logger.info(\"This INFO should not appear (level is ERROR)\")\n",
    "    updated_logger.error(\"This ERROR should appear\")\n",
    "    \n",
    "    print(\"✅ Successfully updated logger configuration\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error updating logger: {e}\")\n",
    "\n",
    "# DELETE: Remove a logger\n",
    "print(\"\\n=== DELETING LOGGER ===\")\n",
    "\n",
    "try:\n",
    "    log_manager.remove_logger(\"test_database\")\n",
    "    print(\"✅ Successfully removed 'test_database' logger\")\n",
    "    print(f\"Remaining loggers: {list(log_manager._loggers_map.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error removing logger: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc15ad",
   "metadata": {},
   "source": [
    "## 3. Create and Manage Multiple Handlers\n",
    "\n",
    "Test CRUD operations for different handler types and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e421fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING HANDLER CRUD OPERATIONS ===\n",
      "✅ Created custom file handler\n",
      "✅ Created custom console handler\n",
      "All handlers: ['handler_file', 'handler_console', 'custom_file_handler', 'custom_console_handler']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ⚠️ The format referenced by handler 'custom_file_handler' is not defined in the 'formats' section of the config file. Using the format as is: \n",
      "\t format_detailed \n",
      "\n",
      " ⚠️ The format referenced by handler 'custom_console_handler' is not defined in the 'formats' section of the config file. Using the format as is: \n",
      "\t format_simple \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Handler CRUD Operations\n",
    "print(\"=== TESTING HANDLER CRUD OPERATIONS ===\")\n",
    "\n",
    "# CREATE: Add custom handlers\n",
    "try:\n",
    "    # Add a custom file handler\n",
    "    custom_file_handler = {\n",
    "        \"sink\": \"test_custom.log\",\n",
    "        \"level\": \"DEBUG\",\n",
    "        \"format\": \"format_detailed\",\n",
    "        \"rotation\": \"10 MB\"\n",
    "    }\n",
    "    \n",
    "    log_manager.add_handler(\"custom_file_handler\", custom_file_handler)\n",
    "    print(\"✅ Created custom file handler\")\n",
    "    \n",
    "    # Add a custom console handler\n",
    "    custom_console_handler = {\n",
    "        \"sink\": \"sys.stderr\",\n",
    "        \"level\": \"WARNING\",\n",
    "        \"format\": \"format_simple\"\n",
    "    }\n",
    "    \n",
    "    log_manager.add_handler(\"custom_console_handler\", custom_console_handler)\n",
    "    print(\"✅ Created custom console handler\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating handlers: {e}\")\n",
    "\n",
    "print(f\"All handlers: {list(log_manager._handlers_map.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c39e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UPDATING HANDLER CONFIGURATION ===\n",
      "✅ Successfully updated console handler\n",
      "format_detailed\n",
      "format_detailed\n",
      "✅ Successfully created and tested custom logger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ⚠️ The format referenced by handler 'custom_console_handler' is not defined in the 'formats' section of the config file. Using the format as is: \n",
      "\t format_detailed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UPDATE: Modify handler configuration\n",
    "print(\"\\n=== UPDATING HANDLER CONFIGURATION ===\")\n",
    "\n",
    "try:\n",
    "    # Update the custom console handler to INFO level\n",
    "    updated_console_handler = {\n",
    "        \"sink\": \"sys.stdout\",\n",
    "        \"level\": \"INFO\",\n",
    "        \"format\": \"format_detailed\"\n",
    "    }\n",
    "    \n",
    "    log_manager.update_handler(\"custom_console_handler\", updated_console_handler)\n",
    "    print(\"✅ Successfully updated console handler\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error updating handler: {e}\")\n",
    "\n",
    "# CREATE: Add logger that uses new handlers\n",
    "try:\n",
    "    log_manager.add_logger(\"test_custom\", [\n",
    "        {\"handler\": \"custom_file_handler\", \"level\": \"DEBUG\"},\n",
    "        {\"handler\": \"custom_console_handler\", \"level\": \"INFO\"}\n",
    "    ])\n",
    "    \n",
    "    # Test the custom logger\n",
    "    custom_logger = log_manager.get_logger(\"test_custom\")\n",
    "    custom_logger.debug(\"Debug message to custom handlers\")\n",
    "    custom_logger.info(\"Info message to custom handlers\")\n",
    "    custom_logger.warning(\"Warning message to custom handlers\")\n",
    "    \n",
    "    print(\"✅ Successfully created and tested custom logger\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with custom logger: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "434692a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DELETING HANDLERS ===\n",
      "✅ Successfully removed custom file handler\n",
      "Remaining handlers: ['handler_file', 'handler_console', 'custom_console_handler']\n",
      "Remaining loggers: ['default_task', 'test_app', 'test_custom']\n"
     ]
    }
   ],
   "source": [
    "# DELETE: Remove handlers\n",
    "print(\"\\n=== DELETING HANDLERS ===\")\n",
    "\n",
    "try:\n",
    "    # Remove the custom file handler\n",
    "    log_manager.remove_handler(\"custom_file_handler\")\n",
    "    print(\"✅ Successfully removed custom file handler\")\n",
    "    \n",
    "    print(f\"Remaining handlers: {list(log_manager._handlers_map.keys())}\")\n",
    "    print(f\"Remaining loggers: {list(log_manager._loggers_map.keys())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error removing handler: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16690150",
   "metadata": {},
   "source": [
    "## 4. Testing Duplicate File Warnings\n",
    "\n",
    "Create multiple HDFS copy operations that target the same files to test duplicate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d187cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SETTING UP DUPLICATE FILE TEST ENVIRONMENT ===\n",
      "✅ Created test environment at: C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\n",
      "Log files: ['app1.log', 'app2.log', 'shared.log', 'database.log']\n",
      "Log directory: C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\n",
      "HDFS destination: C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\hdfs_destination\n"
     ]
    }
   ],
   "source": [
    "# Setup test environment for duplicate file testing\n",
    "print(\"=== SETTING UP DUPLICATE FILE TEST ENVIRONMENT ===\")\n",
    "\n",
    "# Create temporary directories and files\n",
    "test_dir = Path(tempfile.mkdtemp(prefix=\"logmanager_test_\"))\n",
    "log_dir = test_dir / \"logs\"\n",
    "hdfs_dir = test_dir / \"hdfs_destination\"\n",
    "\n",
    "# Create directories\n",
    "log_dir.mkdir(parents=True)\n",
    "hdfs_dir.mkdir(parents=True)\n",
    "\n",
    "# Create test log files\n",
    "test_files = [\n",
    "    log_dir / \"app1.log\",\n",
    "    log_dir / \"app2.log\", \n",
    "    log_dir / \"shared.log\",\n",
    "    log_dir / \"database.log\"\n",
    "]\n",
    "\n",
    "for file_path in test_files:\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(f\"Test log content for {file_path.name}\\n\")\n",
    "        f.write(f\"Timestamp: {time.time()}\\n\")\n",
    "        f.write(\"This is test log data for duplicate detection testing.\\n\")\n",
    "\n",
    "print(f\"✅ Created test environment at: {test_dir}\")\n",
    "print(f\"Log files: {[f.name for f in test_files]}\")\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(f\"HDFS destination: {hdfs_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceebb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 2 failed for C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\shared.log: Unsupported file format: .log. Supported formats: ['csv', 'txt', 'text', 'sql', 'json', 'yaml', 'yml', 'arrow', 'feather', 'parquet', 'pickle', 'pkl']. Retrying in 5s...\n",
      "Attempt 3 failed for C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\shared.log: Unsupported file format: .log. Supported formats: ['csv', 'txt', 'text', 'sql', 'json', 'yaml', 'yml', 'arrow', 'feather', 'parquet', 'pickle', 'pkl']. Retrying in 5s...\n",
      "Attempt 3 failed for C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\shared.log: Unsupported file format: .log. Supported formats: ['csv', 'txt', 'text', 'sql', 'json', 'yaml', 'yml', 'arrow', 'feather', 'parquet', 'pickle', 'pkl']. Retrying in 5s...\n",
      "Failed to copy C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\shared.log after 4 attempts: Unsupported file format: .log. Supported formats: ['csv', 'txt', 'text', 'sql', 'json', 'yaml', 'yml', 'arrow', 'feather', 'parquet', 'pickle', 'pkl']\n",
      "HDFS copy completed: 0 successful, 2 failed\n",
      "Failed to copy C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\shared.log after 4 attempts: Unsupported file format: .log. Supported formats: ['csv', 'txt', 'text', 'sql', 'json', 'yaml', 'yml', 'arrow', 'feather', 'parquet', 'pickle', 'pkl']\n",
      "Attempt 1 failed for C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\app2.log: Unsupported file format: .log. Supported formats: ['csv', 'txt', 'text', 'sql', 'json', 'yaml', 'yml', 'arrow', 'feather', 'parquet', 'pickle', 'pkl']. Retrying in 5s...\n",
      "HDFS copy 'copy_all_logs' found 4 files to copy.\n",
      "WARNING: copy operation 'copy_all_logs' and 'copy_app_logs' are both copying 3 file(s):\n",
      "  - C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\app1.log\n",
      "  - C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\app2.log\n",
      "  - C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\shared.log\n",
      "This may cause race conditions or unnecessary resource usage. Consider adjusting your copy operation patterns to avoid overlaps.\n",
      "\n",
      "WARNING: copy operation 'copy_all_logs' and 'copy_shared_logs' are both copying 2 file(s):\n",
      "  - C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\database.log\n",
      "  - C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\shared.log\n",
      "This may cause race conditions or unnecessary resource usage. Consider adjusting your copy operation patterns to avoid overlaps.\n",
      "\n",
      "Attempt 1 failed for C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\database.log: Unsupported file format: .log. Supported formats: ['csv', 'txt', 'text', 'sql', 'json', 'yaml', 'yml', 'arrow', 'feather', 'parquet', 'pickle', 'pkl']. Retrying in 5s...\n",
      "Attempt 2 failed for C:\\Users\\LISATA~1\\AppData\\Local\\Temp\\logmanager_test_092zj4fs\\logs\\app2.log: Unsupported file format: .log. Supported formats: ['csv', 'txt', 'text', 'sql', 'json', 'yaml', 'yml', 'arrow', 'feather', 'parquet', 'pickle', 'pkl']. Retrying in 5s...\n"
     ]
    }
   ],
   "source": [
    "# Test duplicate file detection with overlapping copy operations\n",
    "print(\"\\n=== TESTING DUPLICATE FILE WARNINGS ===\")\n",
    "\n",
    "try:\n",
    "    # Start first copy operation targeting all .log files\n",
    "    log_manager.start_hdfs_copy(\n",
    "        copy_name=\"copy_all_logs\",\n",
    "        path_patterns=[str(log_dir / \"*.log\")],\n",
    "        hdfs_destination=str(hdfs_dir / \"all_logs\"),\n",
    "        copy_interval=30  # 30 seconds interval\n",
    "    )\n",
    "    print(\"✅ Started copy operation 'copy_all_logs'\")\n",
    "    \n",
    "    # Start second copy operation targeting specific files (will overlap)\n",
    "    log_manager.start_hdfs_copy(\n",
    "        copy_name=\"copy_app_logs\", \n",
    "        path_patterns=[\n",
    "            str(log_dir / \"app1.log\"),\n",
    "            str(log_dir / \"app2.log\"),\n",
    "            str(log_dir / \"shared.log\")  # This will cause overlap\n",
    "        ],\n",
    "        hdfs_destination=str(hdfs_dir / \"app_logs\"),\n",
    "        copy_interval=25  # 25 seconds interval\n",
    "    )\n",
    "    print(\"✅ Started copy operation 'copy_app_logs'\")\n",
    "    \n",
    "    # Start third copy operation with more overlap\n",
    "    log_manager.start_hdfs_copy(\n",
    "        copy_name=\"copy_shared_logs\",\n",
    "        path_patterns=[\n",
    "            str(log_dir / \"shared.log\"),    # Overlaps with both above\n",
    "            str(log_dir / \"database.log\")   # Overlaps with first\n",
    "        ],\n",
    "        hdfs_destination=str(hdfs_dir / \"shared_logs\"),\n",
    "        copy_interval=20  # 20 seconds interval\n",
    "    )\n",
    "    print(\"✅ Started copy operation 'copy_shared_logs'\")\n",
    "    \n",
    "    print(\"\\n⚠️  Check the output above for duplicate file warnings!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up duplicate file test: {e}\")\n",
    "\n",
    "# List active copy operations\n",
    "operations = log_manager.list_hdfs_copy_operations()\n",
    "print(f\"\\nActive HDFS copy operations: {len(operations)}\")\n",
    "for op in operations:\n",
    "    print(f\"  - {op['name']}: {op['is_alive']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccddb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment and trigger manual copy to see duplicate warnings immediately\n",
    "print(\"=== TRIGGERING MANUAL COPY TO SEE DUPLICATE WARNINGS ===\")\n",
    "\n",
    "time.sleep(2)  # Let the operations initialize\n",
    "\n",
    "try:\n",
    "    # Trigger all copy operations manually\n",
    "    log_manager.trigger_hdfs_copy_now()\n",
    "    print(\"✅ Triggered all copy operations manually\")\n",
    "    print(\"\\n⚠️  Look for duplicate file warnings in the output above!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error triggering manual copy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f7506",
   "metadata": {},
   "source": [
    "## 5. HDFS Copy Operations\n",
    "\n",
    "Test various HDFS copy scenarios including error handling and different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91103e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different HDFS copy configurations\n",
    "print(\"=== TESTING VARIOUS HDFS COPY CONFIGURATIONS ===\")\n",
    "\n",
    "# Create additional test files with directory structure\n",
    "structured_dir = test_dir / \"structured_logs\"\n",
    "app_dir = structured_dir / \"app\"\n",
    "db_dir = structured_dir / \"database\"\n",
    "\n",
    "app_dir.mkdir(parents=True)\n",
    "db_dir.mkdir(parents=True)\n",
    "\n",
    "# Create files in subdirectories\n",
    "structured_files = [\n",
    "    app_dir / \"app_2024.log\",\n",
    "    app_dir / \"app_errors.log\",\n",
    "    db_dir / \"db_queries.log\",\n",
    "    db_dir / \"db_errors.log\"\n",
    "]\n",
    "\n",
    "for file_path in structured_files:\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(f\"Structured log content for {file_path}\\n\")\n",
    "        f.write(f\"Created at: {time.time()}\\n\")\n",
    "\n",
    "print(f\"✅ Created structured log files\")\n",
    "print(f\"App logs: {[f.name for f in app_dir.glob('*.log')]}\")\n",
    "print(f\"DB logs: {[f.name for f in db_dir.glob('*.log')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test copy with structure preservation\n",
    "print(\"\\n=== TESTING COPY WITH STRUCTURE PRESERVATION ===\")\n",
    "\n",
    "try:\n",
    "    log_manager.start_hdfs_copy(\n",
    "        copy_name=\"copy_with_structure\",\n",
    "        path_patterns=[str(structured_dir / \"**\" / \"*.log\")],\n",
    "        hdfs_destination=str(hdfs_dir / \"structured_copy\"),\n",
    "        root_dir=str(structured_dir),\n",
    "        preserve_structure=True,\n",
    "        copy_interval=40,\n",
    "        create_dest_dirs=True\n",
    "    )\n",
    "    print(\"✅ Started structured copy operation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with structured copy: {e}\")\n",
    "\n",
    "# Test copy without structure preservation\n",
    "try:\n",
    "    log_manager.start_hdfs_copy(\n",
    "        copy_name=\"copy_flat_structure\",\n",
    "        path_patterns=[str(structured_dir / \"**\" / \"*.log\")],\n",
    "        hdfs_destination=str(hdfs_dir / \"flat_copy\"),\n",
    "        preserve_structure=False,\n",
    "        copy_interval=45,\n",
    "        max_retries=2,\n",
    "        retry_delay=3\n",
    "    )\n",
    "    print(\"✅ Started flat copy operation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with flat copy: {e}\")\n",
    "\n",
    "# Show all active operations\n",
    "operations = log_manager.list_hdfs_copy_operations()\n",
    "print(f\"\\nTotal active operations: {len(operations)}\")\n",
    "for op in operations:\n",
    "    print(f\"  - {op['name']}: alive={op['is_alive']}, thread={op['thread_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test manual triggering of specific operations\n",
    "print(\"\\n=== TESTING MANUAL COPY TRIGGERS ===\")\n",
    "\n",
    "try:\n",
    "    # Trigger specific operation\n",
    "    print(\"Triggering 'copy_with_structure' operation...\")\n",
    "    log_manager.trigger_hdfs_copy_now(\"copy_with_structure\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Trigger specific operation\n",
    "    print(\"\\nTriggering 'copy_flat_structure' operation...\")\n",
    "    log_manager.trigger_hdfs_copy_now(\"copy_flat_structure\")\n",
    "    \n",
    "    print(\"\\n✅ Manual triggers completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with manual triggers: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee3634",
   "metadata": {},
   "source": [
    "## 6. Distributed Coordination Testing\n",
    "\n",
    "Test the distributed coordination features using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6de618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test distributed coordination status\n",
    "print(\"=== TESTING DISTRIBUTED COORDINATION ===\")\n",
    "\n",
    "# Check current coordination status\n",
    "try:\n",
    "    status = log_manager.get_hdfs_copy_status()\n",
    "    print(\"Current HDFS Copy Status:\")\n",
    "    print(f\"  Enabled: {status['hdfs_copy_enabled']}\")\n",
    "    print(f\"  Reason: {status['reason']}\")\n",
    "    print(f\"  Environment: {status['environment_variable']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error getting status: {e}\")\n",
    "\n",
    "# Test the internal coordination methods\n",
    "try:\n",
    "    should_run = log_manager._should_run_hdfs_copy()\n",
    "    print(f\"\\nShould run HDFS copy: {should_run}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking coordination: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e25eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test coordination with environment variable changes\n",
    "print(\"\\n=== TESTING COORDINATION WITH ENVIRONMENT CHANGES ===\")\n",
    "\n",
    "# Save original environment\n",
    "original_env = os.environ.get('DISABLE_HDFS_COPY', None)\n",
    "\n",
    "try:\n",
    "    # Test with HDFS copy disabled\n",
    "    print(\"Setting DISABLE_HDFS_COPY=true...\")\n",
    "    os.environ['DISABLE_HDFS_COPY'] = 'true'\n",
    "    \n",
    "    # Create new LogManager to test initialization with disabled coordination\n",
    "    disabled_log_manager = LogManager()\n",
    "    \n",
    "    print(f\"New LogManager HDFS enabled: {disabled_log_manager.hdfs_copy_enabled}\")\n",
    "    \n",
    "    # Try to start HDFS copy (should be skipped)\n",
    "    disabled_log_manager.start_hdfs_copy(\n",
    "        copy_name=\"should_be_skipped\",\n",
    "        path_patterns=[str(log_dir / \"*.log\")],\n",
    "        hdfs_destination=str(hdfs_dir / \"skipped\"),\n",
    "        copy_interval=60\n",
    "    )\n",
    "    \n",
    "    operations = disabled_log_manager.list_hdfs_copy_operations()\n",
    "    print(f\"Operations after disabled start: {len(operations)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing disabled coordination: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Restore original environment\n",
    "    if original_env is None:\n",
    "        os.environ.pop('DISABLE_HDFS_COPY', None)\n",
    "    else:\n",
    "        os.environ['DISABLE_HDFS_COPY'] = original_env\n",
    "    print(\"\\n✅ Environment restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate distributed environment scenarios\n",
    "print(\"\\n=== SIMULATING DISTRIBUTED SCENARIOS ===\")\n",
    "\n",
    "def simulate_worker_node():\n",
    "    \"\"\"Simulate a worker node with HDFS copy disabled\"\"\"\n",
    "    original_env = os.environ.get('DISABLE_HDFS_COPY', None)\n",
    "    \n",
    "    try:\n",
    "        # Set worker environment\n",
    "        os.environ['DISABLE_HDFS_COPY'] = 'true'\n",
    "        \n",
    "        # Create worker LogManager\n",
    "        worker_manager = LogManager()\n",
    "        \n",
    "        # Try to start copy operation\n",
    "        worker_manager.start_hdfs_copy(\n",
    "            copy_name=\"worker_copy\",\n",
    "            path_patterns=[str(log_dir / \"*.log\")],\n",
    "            hdfs_destination=str(hdfs_dir / \"worker\"),\n",
    "            copy_interval=60\n",
    "        )\n",
    "        \n",
    "        status = worker_manager.get_hdfs_copy_status()\n",
    "        operations = worker_manager.list_hdfs_copy_operations()\n",
    "        \n",
    "        return {\n",
    "            'node_type': 'worker',\n",
    "            'hdfs_enabled': status['hdfs_copy_enabled'],\n",
    "            'reason': status['reason'],\n",
    "            'operations_count': len(operations)\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        # Restore environment\n",
    "        if original_env is None:\n",
    "            os.environ.pop('DISABLE_HDFS_COPY', None)\n",
    "        else:\n",
    "            os.environ['DISABLE_HDFS_COPY'] = original_env\n",
    "\n",
    "def simulate_coordinator_node():\n",
    "    \"\"\"Simulate a coordinator node with HDFS copy enabled\"\"\"\n",
    "    # Ensure no disable flag is set\n",
    "    os.environ.pop('DISABLE_HDFS_COPY', None)\n",
    "    \n",
    "    # Create coordinator LogManager\n",
    "    coordinator_manager = LogManager()\n",
    "    \n",
    "    # Start copy operation\n",
    "    coordinator_manager.start_hdfs_copy(\n",
    "        copy_name=\"coordinator_copy\",\n",
    "        path_patterns=[str(log_dir / \"*.log\")],\n",
    "        hdfs_destination=str(hdfs_dir / \"coordinator\"),\n",
    "        copy_interval=60\n",
    "    )\n",
    "    \n",
    "    status = coordinator_manager.get_hdfs_copy_status()\n",
    "    operations = coordinator_manager.list_hdfs_copy_operations()\n",
    "    \n",
    "    # Stop the operation for cleanup\n",
    "    coordinator_manager.stop_hdfs_copy(\"coordinator_copy\")\n",
    "    \n",
    "    return {\n",
    "        'node_type': 'coordinator',\n",
    "        'hdfs_enabled': status['hdfs_copy_enabled'],\n",
    "        'reason': status['reason'],\n",
    "        'operations_count': len(operations)\n",
    "    }\n",
    "\n",
    "# Run simulations\n",
    "try:\n",
    "    worker_result = simulate_worker_node()\n",
    "    coordinator_result = simulate_coordinator_node()\n",
    "    \n",
    "    print(\"Simulation Results:\")\n",
    "    print(f\"Worker Node:\")\n",
    "    print(f\"  - HDFS Enabled: {worker_result['hdfs_enabled']}\")\n",
    "    print(f\"  - Reason: {worker_result['reason']}\")\n",
    "    print(f\"  - Operations: {worker_result['operations_count']}\")\n",
    "    \n",
    "    print(f\"\\nCoordinator Node:\")\n",
    "    print(f\"  - HDFS Enabled: {coordinator_result['hdfs_enabled']}\")\n",
    "    print(f\"  - Reason: {coordinator_result['reason']}\")\n",
    "    print(f\"  - Operations: {coordinator_result['operations_count']}\")\n",
    "    \n",
    "    print(\"\\n✅ Distributed coordination simulation completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in simulation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee4da7",
   "metadata": {},
   "source": [
    "## 7. Cleanup and Resource Management\n",
    "\n",
    "Properly stop all operations and clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all HDFS copy operations\n",
    "print(\"=== CLEANUP: STOPPING ALL HDFS OPERATIONS ===\")\n",
    "\n",
    "try:\n",
    "    # Stop all operations with verbose output\n",
    "    failed_operations = log_manager.stop_all_hdfs_copy(timeout=15.0, verbose=True)\n",
    "    \n",
    "    if failed_operations:\n",
    "        print(f\"\\n⚠️  Some operations failed to stop: {failed_operations}\")\n",
    "    else:\n",
    "        print(\"\\n✅ All HDFS operations stopped successfully\")\n",
    "        \n",
    "    # Verify no operations are running\n",
    "    remaining_operations = log_manager.list_hdfs_copy_operations()\n",
    "    print(f\"Remaining operations: {len(remaining_operations)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during operation cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f263b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up test files and directories\n",
    "print(\"\\n=== CLEANUP: REMOVING TEST FILES ===\")\n",
    "\n",
    "try:\n",
    "    # Remove temporary test directory\n",
    "    if test_dir.exists():\n",
    "        shutil.rmtree(test_dir)\n",
    "        print(f\"✅ Removed test directory: {test_dir}\")\n",
    "    \n",
    "    # Remove any log files created in current directory\n",
    "    current_dir = Path.cwd()\n",
    "    log_files = list(current_dir.glob(\"test_*.log\"))\n",
    "    \n",
    "    for log_file in log_files:\n",
    "        try:\n",
    "            log_file.unlink()\n",
    "            print(f\"✅ Removed log file: {log_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not remove {log_file.name}: {e}\")\n",
    "    \n",
    "    print(\"\\n✅ File cleanup completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during file cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9842ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status and summary\n",
    "print(\"\\n=== FINAL TEST SUMMARY ===\")\n",
    "\n",
    "try:\n",
    "    # Get final status\n",
    "    final_status = log_manager.get_hdfs_copy_status()\n",
    "    final_operations = log_manager.list_hdfs_copy_operations()\n",
    "    \n",
    "    print(\"LogManager Final State:\")\n",
    "    print(f\"  - Handlers: {len(log_manager._handlers_map)}\")\n",
    "    print(f\"  - Loggers: {len(log_manager._loggers_map)}\")\n",
    "    print(f\"  - HDFS Copy Enabled: {final_status['hdfs_copy_enabled']}\")\n",
    "    print(f\"  - Active Operations: {len(final_operations)}\")\n",
    "    \n",
    "    print(\"\\n🎉 COMPREHENSIVE TESTING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"\\nTests covered:\")\n",
    "    print(\"  ✅ Logger CRUD operations (Create, Read, Update, Delete)\")\n",
    "    print(\"  ✅ Handler CRUD operations\")\n",
    "    print(\"  ✅ Duplicate file warning detection\")\n",
    "    print(\"  ✅ HDFS copy operations with various configurations\")\n",
    "    print(\"  ✅ Distributed coordination with environment variables\")\n",
    "    print(\"  ✅ Manual copy triggering\")\n",
    "    print(\"  ✅ Resource cleanup and management\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error getting final status: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LogManager testing completed. Review the output above for any warnings or errors.\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
